# Thanhnien Article Ingestion Design

## Goals
- Convert entries in `data/thanhnien_jobs.ndjson` into fully populated article records in PostgreSQL.
- Persist referenced images and videos to deterministic filesystem locations and store their paths in the database.
- Make the pipeline resilient (dedupe, retries, logging) and easy to extend or re-run incrementally.

## Out of Scope
- Real-time crawling of newly published content.
- Full-text search indexing or content enrichment beyond parsing the Thanhnien article page.
- Multi-tenant asset hosting (only local filesystem storage covered here).

## Inputs and Outputs
- **Input**: NDJSON lines with `url`, `lastmod`, `sitemap_url`, and optional `image_url` fields generated by `crawler/sitemap_backfill.py`.
- **Output**: Records written to the `articles`, `article_images`, and `article_videos` tables via SQLAlchemy models in `models.py`, along with files written to the ingestion filesystem (`storage/articles/...`).
- **Intermediate**: Raw HTML and metadata cached per article to ease debugging and replay; stored under `storage/raw/{article_uuid}.html` (optional but recommended).

## High-Level Flow
1. **Job Loader** reads NDJSON stream and emits crawl jobs that pass dedupe checks.
2. **Fetcher** downloads article HTML with retry/backoff and validates content type and response codes.
3. **Parser** (Thanhnien-specific) extracts structured fields (title, description, publish date, body blocks, tags, category, embedded media URLs, comments snapshot).
4. **Asset Pipeline** packages referenced media into Celery jobs and defers heavy downloads to a dedicated worker so ingestion remains snappy.
5. **Persistence Layer** upserts the article metadata and associated media paths into PostgreSQL through SQLAlchemy models, handling transactions and sequence numbering.
6. **Bookkeeping** records crawl status, failures, and asset hashes to support reruns without duplicate work.
7. **Celery Worker** consumes queued asset jobs, performs the actual downloads, and finalises persistence.

## Component Design

### 1. Job Management
- Module: `crawler/jobs.py`
- Responsibilities: stream NDJSON (`Path("data/thanhnien_jobs.ndjson")`), validate URLs, skip entries already persisted (check DB) or previously failed beyond retry limit (tracked in local SQLite or JSONL state).
- Emits `ArticleJob` dataclass with `url`, `lastmod`, `priority`, and dedupe key.

### 2. HTTP Fetcher
- Module: `crawler/http_client.py`
- Use `httpx` (sync) with connection pooling, 5s timeout, retry policy (max 3 attempts, exponential backoff, respect 429/503 Retry-After).
- Identify paywalled or redirected pages; emit typed failure events for logging and metrics.
- Persist raw HTML to `storage/raw/{uuid}.html` when fetch succeeds (controlled by config flag for space).

### 3. Parser / Normaliser
- Module: `crawler/parsers/thanhnien.py`
- Separate HTML parsing from business logic. Responsibilities:
  - Parse DOM with `selectolax` or `BeautifulSoup` to extract title, subtitle, description, body paragraphs, publish date, authors, tags, category breadcrumbs, canonical URL.
  - Capture image and video blocks in article order, returning `ParsedAsset` list with source URL, type, caption, sequence.
  - Normalise publish date to timezone-aware `datetime`, map categories to internal IDs.
  - Provide data model `ParsedArticle` with fields matching `models.Article` (site slug supplied by orchestrator) plus an `assets` array.
- Parser should be deterministic and idempotent; unit tests will live under `tests/parsers/test_thanhnien.py` using stored HTML fixtures.

### 4. Asset Pipeline
- Module: `crawler/assets.py`
- Inputs: `ParsedAsset` list and article UUID.
- Steps:
  1. Filter out inline data URIs or duplicates (based on URL or content hash).
  2. Serialize assets into queue payloads (`assets_to_payload`) and enqueue them via Celery so the downloader can run out-of-process.
  3. Worker side (see §4.1) resolves manifests, streams binaries with 30s timeout, and writes to deterministic paths.
  4. Compute checksum (SHA256) and byte size while streaming to disk; surface these in the stored asset metadata for audit and persistence layers.
- Returns `ArticleImage` / `ArticleVideo` rows via the worker, keeping the ingestion loop fast and fault-tolerant.

#### 4.1 Celery Download Task
- Module: `crawler/tasks.py`
- Task name: `crawler.download_assets`
- Executes the former synchronous `AssetManager.download_assets` logic with retry/backoff, persists results through `ArticlePersistence.persist_assets`, and reports status for monitoring.
- Runs with database-backed broker/result store so tasks survive worker restarts.

### 5. Persistence Layer
- Module: `crawler/persistence.py`
- Responsibilities:
- Manage SQLAlchemy sessions (reuse engine from app config).
- Upsert on `Article.url` to avoid duplicates. Use `session.merge` or explicit `ON CONFLICT` if switching to SQLModel/async later.
- Persist `Article.site_slug` with the active site identifier so multi-site crawls can be filtered downstream.
  - Maintain `sequence_number` for assets based on parser output order.
  - Wrap operations in a transaction; rollback and mark job as failed on exceptions.
  - Record crawl metadata (fetch timestamp, HTTP status) in `Article.comments` JSON for traceability.

### 6. Orchestrator / CLI
- Module: `crawler/ingest.py` (multi-site entrypoint; `crawler/ingest_thanhnien.py` wraps it for compatibility)
- CLI Example:
  ```bash
  # add --resume if interrupt from previous run
  python -m crawler.ingest \
      --site thanhnien \
      --jobs-file data/thanhnien_jobs.ndjson \
      --storage-root storage \
      --max-workers 4 \
      --db-url postgresql://crawl_user:crawl_password@localhost:5433/crawl_db \
      --use-playwright
  ```
- Orchestrates worker pool:
  1. Load jobs
  2. Submit to worker threads (ThreadPool for I/O bound tasks)
  3. Each worker executes Fetcher → Parser → enqueue asset download tasks
  4. Collect metrics (success/fail counts, duration) printed at end while asset downloads continue asynchronously
- Supports `--resume` flag to skip jobs already marked success in state store.
- Proxy controls: configure `--proxy ip:port[:key]` plus `--proxy-change-url`/`--proxy-key` to rotate after block responses; rotation calls throttle to 240s by default.

## Data Storage Layout
- `storage/raw/{article_uuid}.html` (optional)
- `storage/articles/{article_uuid}/images/{sequence:03d}.{ext}`
- `storage/articles/{article_uuid}/videos/{sequence:03d}.{ext}`
- The relative path stored in DB should exclude the storage root (e.g., `articles/{uuid}/images/001.jpg`).
- A lightweight manifest (`storage/articles/{article_uuid}/metadata.json`) can capture crawler metadata for offline inspection.

## Failure Handling & Observability
- Retries with exponential backoff for network errors; hard-fail on 404/410 with reason stored in job state.
- Jobs capped at 3 attempts; failures written to `storage/logs/ingest_failures.ndjson` with detailed context.
- Fetch attempts that exhaust retries append a JSON line to `storage/logs/fetch_failures.ndjson` capturing URL, sitemap source, lastmod, error string, and timestamp for follow-up runs.
- Structured logging via `structlog` to make it easy to ship logs later.
- Metrics counters (success, failure, skipped, asset download failures) aggregated and printed on exit.

## Configuration
- `crawler/config.py` provides a dataclass loaded from `ingest.yaml` or env vars:
  - `db_url`
  - `storage_root`
  - `max_workers`
  - `request_timeout`, `asset_timeout`
  - `respect_robots` flag (future work)
  - `raw_html_cache_enabled`
- Proxy configuration (proxy endpoint, rotation API URL, minimum rotation interval).
- Config overrides via CLI flags for quick experimentation.

## Testing Strategy
- **Parser Unit Tests** using saved HTML fixtures to validate extraction logic.
- **Integration Tests** with VCR.py to record one or two sample pages and replay fetch + parse + persist.
- **Dry-run Mode** in CLI that performs fetch/parse but skips persistence, logging the results for manual inspection.
- **Database Tests**: reuse existing docker-compose to spin up Postgres; run ingestion on a tiny fixture set to ensure tables populated correctly.
- **Asset Tests**: use temporary directory and mocked HTTP responses (`httpx.MockTransport`) to validate file naming, checksum, byte counts, and error handling for empty/failed downloads.
- **Model Smoke Test** (`test_models.py`): requires `DATABASE_URL` env var (defaults to `postgresql://crawl_user:crawl_password@localhost:5432/crawl_db`) when running via `python -m unittest discover`. `export DATABASE_URL=postgresql://crawl_user:crawl_password@localhost:5433/crawl_db `

## Operational Considerations
- Rate limiting: default concurrency of 4, per-domain delay (500ms) to be a good citizen.
- Idempotency: job key is article URL; successful runs mark job complete so reruns skip quickly.
- Backpressure: asset download failures cause the Celery task to retry with exponential backoff; ingestion logs the queue submission so operators can correlate.
- Disk usage monitoring: optional CLI flag `--max-bytes-per-article` to warn if asset payload exceeds quota.

## Running with Celery, RabbitMQ, and PostgreSQL
- Copy `.env.sample` to `.env` (tweak values as needed) so Docker Compose can inject service credentials and URLs.
- Set `CRAWLER_DATABASE_URL` to the same SQLAlchemy DSN used by ingestion (e.g., `postgresql://crawl_user:crawl_password@postgres:5432/crawl_db` when running inside Docker). The Celery app will derive both broker (`sqla+...`) and result backend (`db+...`) from it automatically if overrides are not provided.
- Install requirements: `pip install -r requirements.txt` (ensure `ffmpeg` is available on the PATH for HLS downloads).
- Start a worker:
  ```bash
  docker compose run --rm test_app \
    python -m celery -A crawler.celery_app worker --loglevel=info
  ```
- When running via Docker Compose use service hostnames (e.g., `postgres`, `rabbitmq`) and bind mount `./storage:/app/storage` so downloaded assets persist on the host.
- Launch the ingestion CLI in a separate process; asset downloads will be processed asynchronously by the worker:
  ```bash
  docker compose run --rm test_app \
    python -m crawler.ingest \
      --site thanhnien \
      --jobs-file data/thanhnien_jobs.ndjson \
      --storage-root /app/storage \
      --max-workers 4 \
      --db-url postgresql://crawl_user:crawl_password@postgres:5432/crawl_db
  ```
- To clear outstanding jobs, purge the queue via `docker compose exec rabbitmq rabbitmqctl purge_queue celery` before re-running ingestion.

## Next Implementation Steps
1. Validate Celery queue behaviour with integration tests that enqueue fake assets and assert persistence.
2. Implement parser with fixtures and tests.
3. Expand asset downloader coverage with streaming mocks and failure handling cases.
4. Wire orchestrator CLI and add integration test hitting recorded fixtures.
5. Extend docker-compose workflow to run ingestion end-to-end against a sample job list with the worker running alongside.

## Improvement
- **Ingestion concurrency**: the CLI now drives a worker pool via `config.rate_limit.max_workers`; profile queue depth and tune per-domain throttling if the upstream site starts blocking parallel fetches.
- **Playwright isolation**: HLS manifest resolution remains in-line with ingestion, so a single slow video blocks the whole run. Offload Playwright lookups to a separate queue or background worker so the main loop keeps advancing.
