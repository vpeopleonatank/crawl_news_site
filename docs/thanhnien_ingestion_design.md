# Thanhnien Article Ingestion Design

## Goals
- Convert entries in `data/thanhnien_jobs.ndjson` into fully populated article records in PostgreSQL.
- Persist referenced images and videos to deterministic filesystem locations and store their paths in the database.
- Make the pipeline resilient (dedupe, retries, logging) and easy to extend or re-run incrementally.

## Out of Scope
- Real-time crawling of newly published content.
- Full-text search indexing or content enrichment beyond parsing the Thanhnien article page.
- Multi-tenant asset hosting (only local filesystem storage covered here).

## Inputs and Outputs
- **Input**: NDJSON lines with `url`, `lastmod`, `sitemap_url`, and optional `image_url` fields generated by `crawler/sitemap_backfill.py`.
- **Output**: Records written to the `articles`, `article_images`, and `article_videos` tables via SQLAlchemy models in `models.py`, along with files written to the ingestion filesystem (`storage/articles/...`).
- **Intermediate**: Raw HTML and metadata cached per article to ease debugging and replay; stored under `storage/raw/{article_uuid}.html` (optional but recommended).

## High-Level Flow
1. **Job Loader** reads NDJSON stream and emits crawl jobs that pass dedupe checks.
2. **Fetcher** downloads article HTML with retry/backoff and validates content type and response codes.
3. **Parser** (Thanhnien-specific) extracts structured fields (title, description, publish date, body blocks, tags, category, embedded media URLs, comments snapshot).
4. **Asset Pipeline** downloads referenced images and videos, normalises file formats, and stores them under a predictable directory keyed by the article UUID and media sequence.
5. **Persistence Layer** upserts the article metadata and associated media paths into PostgreSQL through SQLAlchemy models, handling transactions and sequence numbering.
6. **Bookkeeping** records crawl status, failures, and asset hashes to support reruns without duplicate work.

## Component Design

### 1. Job Management
- Module: `crawler/jobs.py`
- Responsibilities: stream NDJSON (`Path("data/thanhnien_jobs.ndjson")`), validate URLs, skip entries already persisted (check DB) or previously failed beyond retry limit (tracked in local SQLite or JSONL state).
- Emits `ArticleJob` dataclass with `url`, `lastmod`, `priority`, and dedupe key.

### 2. HTTP Fetcher
- Module: `crawler/http_client.py`
- Use `httpx` (sync) with connection pooling, 5s timeout, retry policy (max 3 attempts, exponential backoff, respect 429/503 Retry-After).
- Identify paywalled or redirected pages; emit typed failure events for logging and metrics.
- Persist raw HTML to `storage/raw/{uuid}.html` when fetch succeeds (controlled by config flag for space).

### 3. Parser / Normaliser
- Module: `crawler/parsers/thanhnien.py`
- Separate HTML parsing from business logic. Responsibilities:
  - Parse DOM with `selectolax` or `BeautifulSoup` to extract title, subtitle, description, body paragraphs, publish date, authors, tags, category breadcrumbs, canonical URL.
  - Capture image and video blocks in article order, returning `ParsedAsset` list with source URL, type, caption, sequence.
  - Normalise publish date to timezone-aware `datetime`, map categories to internal IDs.
  - Provide data model `ParsedArticle` with fields matching `models.Article` plus `assets` array.
- Parser should be deterministic and idempotent; unit tests will live under `tests/parsers/test_thanhnien.py` using stored HTML fixtures.

### 4. Asset Pipeline
- Module: `crawler/assets.py`
- Inputs: `ParsedAsset` list and article UUID.
- Steps:
  1. Filter out inline data URIs or duplicates (based on URL or content hash).
  2. Download binary content with streaming requests and 30s timeout.
  3. Detect MIME type (via `python-magic`) and map to file extension.
  4. Resize/convert images if required (e.g., keep original, optionally produce web-optimized copy later).
  5. Write to `storage/articles/{article_uuid}/images/{sequence:03d}.{ext}` or `/videos/{sequence:03d}.{ext}`.
  6. Compute checksum (SHA256) and byte size while streaming to disk; surface these in the stored asset metadata for audit and persistence layers.
- Returns `ArticleImage` / `ArticleVideo` SQLAlchemy instances with populated `image_path`/`video_path` relative to `storage/articles/` base, alongside optional checksum/size columns when schema expands.

### 5. Persistence Layer
- Module: `crawler/persistence.py`
- Responsibilities:
  - Manage SQLAlchemy sessions (reuse engine from app config).
  - Upsert on `Article.url` to avoid duplicates. Use `session.merge` or explicit `ON CONFLICT` if switching to SQLModel/async later.
  - Maintain `sequence_number` for assets based on parser output order.
  - Wrap operations in a transaction; rollback and mark job as failed on exceptions.
  - Record crawl metadata (fetch timestamp, HTTP status) in `Article.comments` JSON for traceability.

### 6. Orchestrator / CLI
- Module: `crawler/ingest_thanhnien.py`
- CLI Example:
  ```bash
  # add --resume if interrupt from previous run
  python -m crawler.ingest_thanhnien \
      --jobs-file data/thanhnien_jobs.ndjson \
      --storage-root storage \
      --max-workers 4 \
      --db-url postgresql://crawl_user:crawl_password@localhost:5433/crawl_db \
      --use-playwright
  ```
- Orchestrates worker pool:
  1. Load jobs
  2. Submit to worker threads (ThreadPool for I/O bound tasks)
  3. Each worker executes Fetcher → Parser → Asset Pipeline → Persistence
  4. Collect metrics (success/fail counts, duration) printed at end
- Supports `--resume` flag to skip jobs already marked success in state store.
- Proxy controls: configure `--proxy ip:port[:key]` plus `--proxy-change-url`/`--proxy-key` to rotate after block responses; rotation calls throttle to 240s by default.

## Data Storage Layout
- `storage/raw/{article_uuid}.html` (optional)
- `storage/articles/{article_uuid}/images/{sequence:03d}.{ext}`
- `storage/articles/{article_uuid}/videos/{sequence:03d}.{ext}`
- The relative path stored in DB should exclude the storage root (e.g., `articles/{uuid}/images/001.jpg`).
- A lightweight manifest (`storage/articles/{article_uuid}/metadata.json`) can capture crawler metadata for offline inspection.

## Failure Handling & Observability
- Retries with exponential backoff for network errors; hard-fail on 404/410 with reason stored in job state.
- Jobs capped at 3 attempts; failures written to `storage/logs/ingest_failures.ndjson` with detailed context.
- Fetch attempts that exhaust retries append a JSON line to `storage/logs/fetch_failures.ndjson` capturing URL, sitemap source, lastmod, error string, and timestamp for follow-up runs.
- Structured logging via `structlog` to make it easy to ship logs later.
- Metrics counters (success, failure, skipped, asset download failures) aggregated and printed on exit.

## Configuration
- `crawler/config.py` provides a dataclass loaded from `ingest.yaml` or env vars:
  - `db_url`
  - `storage_root`
  - `max_workers`
  - `request_timeout`, `asset_timeout`
  - `respect_robots` flag (future work)
  - `raw_html_cache_enabled`
- Proxy configuration (proxy endpoint, rotation API URL, minimum rotation interval).
- Config overrides via CLI flags for quick experimentation.

## Testing Strategy
- **Parser Unit Tests** using saved HTML fixtures to validate extraction logic.
- **Integration Tests** with VCR.py to record one or two sample pages and replay fetch + parse + persist.
- **Dry-run Mode** in CLI that performs fetch/parse but skips persistence, logging the results for manual inspection.
- **Database Tests**: reuse existing docker-compose to spin up Postgres; run ingestion on a tiny fixture set to ensure tables populated correctly.
- **Asset Tests**: use temporary directory and mocked HTTP responses (`httpx.MockTransport`) to validate file naming, checksum, byte counts, and error handling for empty/failed downloads.
- **Model Smoke Test** (`test_models.py`): requires `DATABASE_URL` env var (defaults to `postgresql://crawl_user:crawl_password@localhost:5432/crawl_db`) when running via `python -m unittest discover`. `export DATABASE_URL=postgresql://crawl_user:crawl_password@localhost:5433/crawl_db `

## Operational Considerations
- Rate limiting: default concurrency of 4, per-domain delay (500ms) to be a good citizen.
- Idempotency: job key is article URL; successful runs mark job complete so reruns skip quickly.
- Backpressure: if asset download fails, mark job partial and retry later rather than committing incomplete article.
- Disk usage monitoring: optional CLI flag `--max-bytes-per-article` to warn if asset payload exceeds quota.

## Next Implementation Steps
1. Scaffold modules (`jobs`, `http_client`, `parsers`, `assets`, `persistence`, `ingest_thanhnien`).
2. Implement parser with fixtures and tests.
3. Build asset downloader with streaming writes and checksum support.
4. Wire orchestrator CLI and add integration test hitting recorded fixtures.
5. Extend docker-compose workflow to run ingestion end-to-end against a sample job list.
